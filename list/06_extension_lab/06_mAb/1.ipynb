{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## **1. 进一步优化 VAE 与 CVAE 模型**\n",
    "\n",
    "### **VAE 模型改进**\n",
    "\n",
    "1. **Batch Normalization 和 Dropout**：增强泛化能力。\n",
    "2. **改进的损失函数**：考虑 KL 散度与重构误差的平衡。\n",
    "3. **多层网络**：增加模型复杂性以提升表示能力。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d6a454ca12a2242"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d42bc45a6770a705"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, cond_dim):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        \n",
    "        # 编码器\n",
    "        self.fc1 = nn.Linear(input_dim + cond_dim, hidden_dim)  # 注意这里的输入维度要匹配拼接后的维度\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)  # 均值\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)  # 方差\n",
    "\n",
    "        # 解码器\n",
    "        self.fc3 = nn.Linear(latent_dim + cond_dim, hidden_dim)  # 解码时也要拼接条件\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def encode(self, x, cond):\n",
    "        # 拼接输入和条件\n",
    "        x_cond = torch.cat([x, cond], dim=-1)\n",
    "        h1 = torch.relu(self.bn1(self.fc1(x_cond)))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, cond):\n",
    "        # 拼接潜在向量和条件\n",
    "        z_cond = torch.cat([z, cond], dim=-1)\n",
    "        h3 = torch.relu(self.bn2(self.fc3(z_cond)))\n",
    "        h3 = self.dropout(h3)\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        mu, logvar = self.encode(x, cond)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z, cond), mu, logvar\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T09:58:00.044420Z",
     "start_time": "2024-10-23T09:58:00.035948Z"
    }
   },
   "id": "554b21f517fa5c9f",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 数据准备\n",
    "data = torch.randn(32, 100)  # 32 条样本，每条 100 维\n",
    "labels = torch.nn.functional.one_hot(torch.randint(0, 5, (32,)), num_classes=5).float()  # 生成 one-hot 标签\n",
    "\n",
    "# 创建数据集和 DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 初始化模型\n",
    "cvae_model = ConditionalVAE(input_dim=100, hidden_dim=50, latent_dim=10, cond_dim=5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T09:58:00.353422Z",
     "start_time": "2024-10-23T09:58:00.347379Z"
    }
   },
   "id": "c607001a010458d7",
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. \n",
    "\n",
    "### **CVAE 模型实现**\n",
    "\n",
    "1. **条件输入**：嵌入特定标签以生成具有条件约束的序列。\n",
    "2. **标签与特征连接**：将标签与原始输入连接以进行编码。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7cef1dbf57b4402"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-23T09:58:10.374833Z",
     "start_time": "2024-10-23T09:58:10.369665Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, dataloader, epochs=100):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, cond in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            recon_x, mu, logvar = model(x, cond)  # 正确传入条件\n",
    "            loss = ((x - recon_x) ** 2).sum() + 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **2. 训练与比较 VAE 和 CVAE**\n",
    "\n",
    "### **训练函数**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5853840f2f4bb01c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_sequences(model, num_sequences=100, cond=None):\n",
    "    sequences = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_sequences):\n",
    "            z = torch.randn(1, model.fc21.out_features)  # 使用潜在向量生成\n",
    "            generated = model.decode(z, cond).numpy().flatten()\n",
    "            sequence = ''.join(random.choices(\"ACDEFGHIKLMNPQRSTVWY\", k=len(generated)))\n",
    "            sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "def evaluate_sequences(sequences):\n",
    "    diversity = len(set(sequences)) / len(sequences)\n",
    "    stability = np.mean([sum(1 for aa in seq if aa in \"AILMFWV\") / len(seq) for seq in sequences])\n",
    "    return diversity, stability\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T09:58:24.110514Z",
     "start_time": "2024-10-23T09:58:24.104678Z"
    }
   },
   "id": "58a7fcb982023c0e",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3780.9712\n",
      "Epoch 2, Loss: 3760.2236\n",
      "Epoch 3, Loss: 3758.2026\n",
      "Epoch 4, Loss: 3732.9578\n",
      "Epoch 5, Loss: 3707.5842\n",
      "Epoch 6, Loss: 3688.3037\n",
      "Epoch 7, Loss: 3680.5059\n",
      "Epoch 8, Loss: 3644.3115\n",
      "Epoch 9, Loss: 3634.7539\n",
      "Epoch 10, Loss: 3613.0684\n",
      "Epoch 11, Loss: 3587.0449\n",
      "Epoch 12, Loss: 3574.7756\n",
      "Epoch 13, Loss: 3531.6206\n",
      "Epoch 14, Loss: 3541.4749\n",
      "Epoch 15, Loss: 3512.5715\n",
      "Epoch 16, Loss: 3471.8289\n",
      "Epoch 17, Loss: 3441.1858\n",
      "Epoch 18, Loss: 3451.1194\n",
      "Epoch 19, Loss: 3414.0679\n",
      "Epoch 20, Loss: 3382.2461\n",
      "Epoch 21, Loss: 3346.7485\n",
      "Epoch 22, Loss: 3349.8340\n",
      "Epoch 23, Loss: 3298.3862\n",
      "Epoch 24, Loss: 3279.8489\n",
      "Epoch 25, Loss: 3247.9692\n",
      "Epoch 26, Loss: 3181.2300\n",
      "Epoch 27, Loss: 3133.9131\n",
      "Epoch 28, Loss: 3087.9595\n",
      "Epoch 29, Loss: 3020.1404\n",
      "Epoch 30, Loss: 2889.3647\n",
      "Epoch 31, Loss: 2828.0889\n",
      "Epoch 32, Loss: 2617.7615\n",
      "Epoch 33, Loss: 2403.8254\n",
      "Epoch 34, Loss: 2061.0054\n",
      "Epoch 35, Loss: 1576.6433\n",
      "Epoch 36, Loss: 864.0876\n",
      "Epoch 37, Loss: -114.1929\n",
      "Epoch 38, Loss: -1463.3604\n",
      "Epoch 39, Loss: -3265.8691\n",
      "Epoch 40, Loss: -5960.7534\n",
      "Epoch 41, Loss: -9959.0127\n",
      "Epoch 42, Loss: -15586.0039\n",
      "Epoch 43, Loss: -23594.0078\n",
      "Epoch 44, Loss: -34920.8984\n",
      "Epoch 45, Loss: -50859.6719\n",
      "Epoch 46, Loss: -73595.3359\n",
      "Epoch 47, Loss: -105558.2500\n",
      "Epoch 48, Loss: -150402.9531\n",
      "Epoch 49, Loss: -213195.8906\n",
      "Epoch 50, Loss: -300103.2500\n",
      "Epoch 51, Loss: -420476.5625\n",
      "Epoch 52, Loss: -586651.2500\n",
      "Epoch 53, Loss: -818528.3750\n",
      "Epoch 54, Loss: -1137995.0000\n",
      "Epoch 55, Loss: -1574628.8750\n",
      "Epoch 56, Loss: -2167594.7500\n",
      "Epoch 57, Loss: -2969108.2500\n",
      "Epoch 58, Loss: -4048725.5000\n",
      "Epoch 59, Loss: -5496755.0000\n",
      "Epoch 60, Loss: -7429263.0000\n",
      "Epoch 61, Loss: -9997131.0000\n",
      "Epoch 62, Loss: -13397019.0000\n",
      "Epoch 63, Loss: -17885048.0000\n",
      "Epoch 64, Loss: -23792940.0000\n",
      "Epoch 65, Loss: -31555136.0000\n",
      "Epoch 66, Loss: -41740212.0000\n",
      "Epoch 67, Loss: -55094764.0000\n",
      "Epoch 68, Loss: -72617408.0000\n",
      "Epoch 69, Loss: -95660976.0000\n",
      "Epoch 70, Loss: -126034952.0000\n",
      "Epoch 71, Loss: -166175088.0000\n",
      "Epoch 72, Loss: -219312464.0000\n",
      "Epoch 73, Loss: -289669408.0000\n",
      "Epoch 74, Loss: -382980064.0000\n",
      "Epoch 75, Loss: -507158144.0000\n",
      "Epoch 76, Loss: -680100736.0000\n",
      "Epoch 77, Loss: -916233216.0000\n",
      "Epoch 78, Loss: -1239447168.0000\n",
      "Epoch 79, Loss: -1682685312.0000\n",
      "Epoch 80, Loss: -2291909632.0000\n",
      "Epoch 81, Loss: -3141448960.0000\n",
      "Epoch 82, Loss: -4355305984.0000\n",
      "Epoch 83, Loss: -6077335552.0000\n",
      "Epoch 84, Loss: -8523772928.0000\n",
      "Epoch 85, Loss: -12001856512.0000\n",
      "Epoch 86, Loss: -16979561472.0000\n",
      "Epoch 87, Loss: -24096495616.0000\n",
      "Epoch 88, Loss: -34343792640.0000\n",
      "Epoch 89, Loss: -49411596288.0000\n",
      "Epoch 90, Loss: -71488159744.0000\n",
      "Epoch 91, Loss: -103870619648.0000\n",
      "Epoch 92, Loss: -151490084864.0000\n",
      "Epoch 93, Loss: -221670948864.0000\n",
      "Epoch 94, Loss: -325440962560.0000\n",
      "Epoch 95, Loss: -479423528960.0000\n",
      "Epoch 96, Loss: -712723333120.0000\n",
      "Epoch 97, Loss: -1075717472256.0000\n",
      "Epoch 98, Loss: -1636068753408.0000\n",
      "Epoch 99, Loss: -2500874207232.0000\n",
      "Epoch 100, Loss: -3840075366400.0000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 50])",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m train(cvae_model, dataloader)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# 生成与评估序列\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m cvae_sequences \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_sequences\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcvae_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_sequences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcond\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# 评估结果\u001B[39;00m\n\u001B[1;32m      8\u001B[0m cvae_diversity, cvae_stability \u001B[38;5;241m=\u001B[39m evaluate_sequences(cvae_sequences)\n",
      "Cell \u001B[0;32mIn[40], line 6\u001B[0m, in \u001B[0;36mgenerate_sequences\u001B[0;34m(model, num_sequences, cond)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_sequences):\n\u001B[1;32m      5\u001B[0m     z \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m1\u001B[39m, model\u001B[38;5;241m.\u001B[39mfc21\u001B[38;5;241m.\u001B[39mout_features)  \u001B[38;5;66;03m# 使用潜在向量生成\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m     generated \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcond\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[1;32m      7\u001B[0m     sequence \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(random\u001B[38;5;241m.\u001B[39mchoices(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mACDEFGHIKLMNPQRSTVWY\u001B[39m\u001B[38;5;124m\"\u001B[39m, k\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(generated)))\n\u001B[1;32m      8\u001B[0m     sequences\u001B[38;5;241m.\u001B[39mappend(sequence)\n",
      "Cell \u001B[0;32mIn[37], line 34\u001B[0m, in \u001B[0;36mConditionalVAE.decode\u001B[0;34m(self, z, cond)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, z, cond):\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;66;03m# 拼接潜在向量和条件\u001B[39;00m\n\u001B[1;32m     33\u001B[0m     z_cond \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([z, cond], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 34\u001B[0m     h3 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbn2\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc3\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz_cond\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     35\u001B[0m     h3 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(h3)\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39msigmoid(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc4(h3))\n",
      "File \u001B[0;32m/opt/anaconda3/envs/teachopencadd/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/teachopencadd/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/teachopencadd/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:175\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    168\u001B[0m     bn_training \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_mean \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_var \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    170\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001B[39;00m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001B[39;00m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 175\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001B[39;49;00m\n\u001B[1;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_mean\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_var\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbn_training\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexponential_average_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/teachopencadd/lib/python3.8/site-packages/torch/nn/functional.py:2480\u001B[0m, in \u001B[0;36mbatch_norm\u001B[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[1;32m   2467\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m   2468\u001B[0m         batch_norm,\n\u001B[1;32m   2469\u001B[0m         (\u001B[38;5;28minput\u001B[39m, running_mean, running_var, weight, bias),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2477\u001B[0m         eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[1;32m   2478\u001B[0m     )\n\u001B[1;32m   2479\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m training:\n\u001B[0;32m-> 2480\u001B[0m     \u001B[43m_verify_batch_size\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2482\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mbatch_norm(\n\u001B[1;32m   2483\u001B[0m     \u001B[38;5;28minput\u001B[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001B[38;5;241m.\u001B[39mbackends\u001B[38;5;241m.\u001B[39mcudnn\u001B[38;5;241m.\u001B[39menabled\n\u001B[1;32m   2484\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/teachopencadd/lib/python3.8/site-packages/torch/nn/functional.py:2448\u001B[0m, in \u001B[0;36m_verify_batch_size\u001B[0;34m(size)\u001B[0m\n\u001B[1;32m   2446\u001B[0m     size_prods \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m size[i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m   2447\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_prods \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m-> 2448\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected more than 1 value per channel when training, got input size \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msize\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 50])"
     ]
    }
   ],
   "source": [
    "# 训练 CVAE\n",
    "train(cvae_model, dataloader)\n",
    "\n",
    "# 生成与评估序列\n",
    "cvae_sequences = generate_sequences(cvae_model, num_sequences=100, cond=torch.tensor([[1, 0, 0, 0, 0]]))\n",
    "\n",
    "# 评估结果\n",
    "cvae_diversity, cvae_stability = evaluate_sequences(cvae_sequences)\n",
    "print(f\"CVAE - Diversity: {cvae_diversity:.2f}, Stability: {cvae_stability:.2f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T09:58:46.749557Z",
     "start_time": "2024-10-23T09:58:46.217912Z"
    }
   },
   "id": "18228c0b213e9e1d",
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "生成与评估函数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd6a8d744978fef4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T09:55:39.914968Z",
     "start_time": "2024-10-23T09:55:39.914784Z"
    }
   },
   "id": "22bb154398a93a3d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3f75ed02b8a829ed",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f8dc4a19059b5ec4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a424462c8926ecce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "26796d77df6b4147"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ea4babff482d1085"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
