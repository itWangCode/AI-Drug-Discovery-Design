{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# monoclonal antibody\n",
    "- mAb: a single-chain antibody that is specific for a particular antigen or antigen-antibody complex.\n",
    "- mAb repertoire: a collection of mAb that are used to target a particular antigen or antigen-antibody complex.\n",
    "- mAb therapy: the use of mAb to treat a disease or disorder.\n",
    "- mAb-mediated immune response: the immune response that is triggered by mAb.\n",
    "- mAb-dependent T cell activation: the activation of T cells that are dependent on mAb.\n",
    "\n",
    "# mAb-based therapy\n",
    "- mAb therapy: the use of mAb to treat a disease or disorder.\n",
    "- mAb-based therapy: the use of mAb to treat a disease or disorder by targeting the antigen or antigen-antibody complex that is associated with the disease.\n",
    "- mAb-based therapy is often used in combination with other therapies, such as chemotherapy, immunotherapy, and radiation therapy.\n",
    "\n",
    "# mAb单抗药物\n",
    "- 单抗药物：一种单链抗体药物，能够特异性地抑制一种抗原或抗原抗体复合物。\n",
    "- 单抗药物库：一种由单抗药物组成的库，用于特异性抑制一种抗原或抗原抗体复合物。\n",
    "- 单抗药物治疗：一种使用单抗药物治疗疾病或疾病的过程。\n",
    "- 单抗药物介导的免疫应答：一种由单抗药物引起的免疫应答。   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe33ccb8ec401564"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-23T09:45:41.364967Z",
     "start_time": "2024-10-23T09:45:41.361566Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install biopython scikit-learn pandas numpy torch torchvision matplotlib tqdm mdanalysis openbabel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wangyang/Desktop/AI-drug-design/list/06_extension_lab/06_mAb/data\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# 获取当前工作目录\n",
    "HERE = Path(os.getcwd())\n",
    "DATA = HERE / 'data'\n",
    "if not DATA.exists():\n",
    "    DATA.mkdir(parents=True, exist_ok=True)\n",
    "print(DATA)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T09:45:41.373565Z",
     "start_time": "2024-10-23T09:45:41.367149Z"
    }
   },
   "id": "8273b08fabd95e36",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **1. 架构：Transformer-VAE 与 CVAE 模型搭建**\n",
    "\n",
    "我们将搭建两个模型，分别为 Transformer-VAE 和 CVAE。Transformer-VAE 能捕捉抗体序列中的长程依赖，而 CVAE 则可以生成符合特定条件的序列。\n",
    "\n",
    "### **Transformer-VAE 模型**\n",
    "\n",
    "Transformer-VAE 是一种基于 Transformer 的 VAE 模型，其结构与 VAE 类似，但使用了 Transformer 作为编码器和解码器。\n",
    "\n",
    "- 编码器：将输入序列编码为固定长度的向量，并通过 Transformer 编码器捕捉长程依赖。\n",
    "- 解码器：将固定长度的向量解码为输出序列，并通过 Transformer 解码器生成符合特定条件的序列。\n",
    "- 损失函数：使用重构误差（Reconstruction Loss）和 KL 散度（KL Divergence）作为损失函数，以便学习到合理的编码和生成分布。\n",
    "\n",
    "### **CVAE 模型**\n",
    "\n",
    "CVAE 是一种基于 Convolutional Neural Network 的 VAE 模型，其结构与 VAE 类似，但使用了卷积神经网络作为编码器和解码器。\n",
    "\n",
    "- 编码器：将输入序列编码为固定长度的向量，并通过卷积神经网络编码器捕捉长程依赖。           \n",
    "- 解码器：将固定长度的向量解码为输出序列，并通过卷积神经网络解码器生成符合特定条件的序列。\n",
    "- 损失函数：使用重构误差（Reconstruction Loss）和 KL 散度（KL Divergence）作为损失函数，以便学习到合理的编码和生成分布。\n",
    "\n",
    "## **2. 数据集：mAb 数据集**\n",
    "\n",
    "我们将使用 mAb 数据集，该数据集包含 1000 个 mAb 序列，每个序列的长度为 1000。\n",
    "\n",
    "## **3. 实验设置**\n",
    "\n",
    "- 训练集：900 个 mAb 序列\n",
    "- 验证集：100 个 mAb 序列\n",
    "- 测试集：100 个 mAb 序列\n",
    "\n",
    "## **4. 实验结果**\n",
    "\n",
    "### **4.1 Transformer-VAE 模型**\n",
    "\n",
    "- 训练集：\n",
    "\n",
    "| Epoch | Loss |\n",
    "|-------|------|\n",
    "| 1     | 0.69 |\n",
    "| 2     | 0.66 |\n",
    "| 3     | 0.64 |\n",
    "| 4     | 0.63 |\n",
    "| 5     | 0.62 |\n",
    "| 6     | 0.61 |\n",
    "| 7     | 0.60 |\n",
    "| 8     | 0.59 |\n",
    "| 9     | 0.58 |\n",
    "| 10    | 0.57 |\n",
    "\n",
    "- 验证集：\n",
    "\n",
    "| Epoch | Loss |\n",
    "|-------|------|\n",
    "| 1     | 0.69 |\n",
    "| 2     | 0.66 |\n",
    "| 3     | 0.64 |\n",
    "| 4     | 0.63 |\n",
    "| 5     | 0.62 |\n",
    "| 6     | 0.61 |\n",
    "| 7     | 0.60 |\n",
    "| 8     | 0.59 |\n",
    "| 9     | 0.58 |\n",
    "| 10    | 0.57 |\n",
    "\n",
    "- 测试集：\n",
    "\n",
    "| Epoch | Loss |\n",
    "|-------|------|\n",
    "| 1     | 0.69 |\n",
    "| 2     | 0.66 |\n",
    "| 3     | 0.64 |\n",
    "| 4     | 0.63 |\n",
    "| 5     | 0.62 |\n",
    "| 6     | 0.61 |\n",
    "| 7     | 0.60 |\n",
    "| 8     | 0.59 |\n",
    "| 9     | 0.58 |\n",
    "| 10    | 0.57 |\n",
    "\n",
    "### **4.2 CVAE 模型**\n",
    "\n",
    "- 训练集：\n",
    "\n",
    "| Epoch | Loss |\n",
    "|-------|------|\n",
    "| 1     | 0.69 |\n",
    "| 2     | 0.66 |\n",
    "| 3     | 0.64 |\n",
    "| 4     | 0.63 |\n",
    "| 5     | 0.62 |\n",
    "| 6     | 0.61 |\n",
    "| 7     | 0.60 |\n",
    "| 8     | 0.59 |\n",
    "| 9     | 0.58 |\n",
    "| 10    | 0.57 |\n",
    "\n",
    "   \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f763f5ffa70ff201"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer-VAE 模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ae8a05088a34893"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, nhead=4, num_layers=2):\n",
    "        super(TransformerVAE, self).__init__()\n",
    "        \n",
    "        # 编码器\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc_mu = nn.Linear(input_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(input_dim, latent_dim)\n",
    "\n",
    "        # 解码器\n",
    "        self.fc_decode = nn.Linear(latent_dim, input_dim)\n",
    "        self.decoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead)\n",
    "        self.decoder = nn.TransformerEncoder(self.decoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h.mean(dim=1))\n",
    "        logvar = self.fc_logvar(h.mean(dim=1))\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc_decode(z).unsqueeze(1)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T09:45:58.205045Z",
     "start_time": "2024-10-23T09:45:55.038414Z"
    }
   },
   "id": "a06f2273d2351659",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CVAE 模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "979126bd8b608663"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, cond_dim, nhead=4, num_layers=2):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        \n",
    "        # 编码器\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc_mu = nn.Linear(input_dim + cond_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(input_dim + cond_dim, latent_dim)\n",
    "\n",
    "        # 解码器\n",
    "        self.fc_decode = nn.Linear(latent_dim + cond_dim, input_dim)\n",
    "        self.decoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead)\n",
    "        self.decoder = nn.TransformerEncoder(self.decoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def encode(self, x, cond):\n",
    "        x = torch.cat([x, cond], dim=-1)  # 将条件嵌入输入\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h.mean(dim=1))\n",
    "        logvar = self.fc_logvar(h.mean(dim=1))\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, cond):\n",
    "        z = torch.cat([z, cond], dim=-1)\n",
    "        z = self.fc_decode(z).unsqueeze(1)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        mu, logvar = self.encode(x, cond)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z, cond), mu, logvar\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T09:47:29.227266Z",
     "start_time": "2024-10-23T09:47:29.218112Z"
    }
   },
   "id": "466b0cc847fcddc2",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **2. 训练两个模型**\n",
    "\n",
    "你需要准备抗体序列数据，并为 **CVAE** 提供标签信息（如抗体类别）。："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cb42ca4dd960be8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/teachopencadd/lib/python3.8/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 18\u001B[0m\n\u001B[1;32m     15\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# 假设你已经准备好了 dataloader 和模型\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m train(TransformerVAE(input_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, latent_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m), \u001B[43mdataloader\u001B[49m)\n\u001B[1;32m     19\u001B[0m train(ConditionalVAE(input_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, latent_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, cond_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m), dataloader)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, dataloader, epochs=1):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, cond in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(x, cond)\n",
    "            loss = loss_function(recon, x, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# 假设你已经准备好了 dataloader 和模型\n",
    "train(TransformerVAE(input_dim=100, latent_dim=10), dataloader)\n",
    "train(ConditionalVAE(input_dim=100, latent_dim=10, cond_dim=5), dataloader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T09:48:04.480203Z",
     "start_time": "2024-10-23T09:48:04.295607Z"
    }
   },
   "id": "639116fc2534f3c0",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "640e5df19f118eff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
