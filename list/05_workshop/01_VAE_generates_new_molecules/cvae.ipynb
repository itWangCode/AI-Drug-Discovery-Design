{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from collections import UserList, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rdkit import RDLogger                                                                                                                                                               \n",
    "RDLogger.DisableLog('rdApp.*') \n",
    "\n",
    "def seed_all():\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HIV dataset\n",
    "hiv_tasks, datasets, transformers = dc.molnet.load_hiv(featurizer=\"ECFP\", set=\"sparse\", splitter = 'random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compound train/valid/test split: 32901/4113/4113\n"
     ]
    }
   ],
   "source": [
    "# Split 80/10/10\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "print(f'Compound train/valid/test split: {len(train_dataset)}/{len(valid_dataset)}/{len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example SMILES: O=C1C(=Cc2ccc(O)cc2)N=C(c2ccccc2)N1n1c(-c2ccccc2)nc2ccccc2c1=O\n",
      "Example label: 0\n"
     ]
    }
   ],
   "source": [
    "# Select SMILES structures and label\n",
    "train_data = list(train_dataset.ids)\n",
    "train_label = list([int(l) for l in train_dataset.y])\n",
    "print(f'Example SMILES: {train_data[0]}')\n",
    "print(f'Example label: {train_label[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set()\n",
    "for string in train_data:\n",
    "    chars.update(string)\n",
    "all_sys = sorted(list(chars)) + ['<bos>', '<eos>', '<pad>', '<unk>']\n",
    "vocab = all_sys\n",
    "c2i = {c: i for i, c in enumerate(all_sys)}\n",
    "i2c = {i: c for i, c in enumerate(all_sys)}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vector = torch.eye(len(c2i))\n",
    "\n",
    "def char2id(char):\n",
    "    if char not in c2i:\n",
    "        return c2i['<unk>']\n",
    "    else:\n",
    "        return c2i[char]\n",
    "\n",
    "def id2char(id):\n",
    "    if id not in i2c:\n",
    "        return i2c[32] \n",
    "    else:\n",
    "        return i2c[id]\n",
    "\n",
    "def string2ids(string,add_bos=False, add_eos=False):\n",
    "    ids = [char2id(c) for c in string]\n",
    "    if add_bos:\n",
    "        ids = [c2i['<bos>']] + ids\n",
    "    if add_eos:\n",
    "        ids = ids + [c2i['<eos>']]\n",
    "    return ids\n",
    "\n",
    "def ids2string(ids, rem_bos=True, rem_eos=True):\n",
    "    if len(ids) == 0:\n",
    "        return ''\n",
    "    if rem_bos and ids[0] == c2i['<bos>']:\n",
    "        ids = ids[1:]\n",
    "    if rem_eos and ids[-1] == c2i['<eos>']:\n",
    "        ids = ids[:-1]\n",
    "    string = ''.join([id2char(id) for id in ids])\n",
    "    return string\n",
    "\n",
    "def string2tensor(string, device='model'):\n",
    "    ids = string2ids(string, add_bos=True, add_eos=True)\n",
    "    tensor = torch.tensor(ids, dtype=torch.long, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    return tensor\n",
    "\n",
    "tensor = [string2tensor(string, device=device) for string in train_data]\n",
    "vector = torch.eye(len(c2i)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "q_bidir = True \n",
    "q_d_h = 256\n",
    "q_n_layers = 1\n",
    "q_dropout = 0.5\n",
    "\n",
    "d_n_layers = 3\n",
    "d_dropout = 0\n",
    "d_z = 128\n",
    "d_d_h = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "  def __init__(self, vocab, vector):\n",
    "    super().__init__()\n",
    "    self.vocabulary = vocab\n",
    "    self.vector = vector\n",
    "    self.n_classes = 2\n",
    "    \n",
    "    n_vocab, d_emb = len(vocab), vector.size(1) # 60, 60\n",
    "    self.x_emb = nn.Embedding(n_vocab, d_emb, padding_idx = c2i['<pad>']) # 60, 60, 58\n",
    "    self.x_emb.weight.data.copy_(vector) # 60, 60\n",
    "  \n",
    "    # Encoder\n",
    "    self.encoder_rnn = nn.GRU(d_emb, q_d_h, num_layers=q_n_layers, batch_first=True, dropout=q_dropout if q_n_layers > 1 else 0, bidirectional=q_bidir) # 60, 256\n",
    "    q_d_last = q_d_h * (2 if q_bidir else 1) # 512\n",
    "    self.q_mu = nn.Linear(q_d_last + self.n_classes, d_z) # 514, 128\n",
    "    self.q_logvar = nn.Linear(q_d_last + self.n_classes, d_z) # 514, 128\n",
    "  \n",
    "    # Decoder\n",
    "    self.decoder_rnn = nn.GRU(d_emb + d_z, d_d_h + self.n_classes, num_layers=d_n_layers, batch_first=True, dropout=d_dropout if d_n_layers > 1 else 0) # 60 + 128, 514\n",
    "    self.decoder_latent = nn.Linear(d_z, d_d_h) # 128, 512\n",
    "    self.decoder_fullyc = nn.Linear(d_d_h + self.n_classes, n_vocab) # 514, 60\n",
    "  \n",
    "  \n",
    "    # Grouping the model's parameters\n",
    "    self.encoder = nn.ModuleList([self.encoder_rnn, self.q_mu, self.q_logvar])\n",
    "    self.decoder = nn.ModuleList([self.decoder_rnn, self.decoder_latent, self.decoder_fullyc])\n",
    "    self.vae = nn.ModuleList([self.x_emb, self.encoder, self.decoder])\n",
    "    \n",
    "\n",
    "  def device(self):\n",
    "    return next(self.parameters()).device\n",
    "\n",
    "  def string2tensor(self, string, label, device='model'):\n",
    "    ids = string2ids(string, add_bos=True, add_eos=True)\n",
    "    tensor = torch.tensor(ids, dtype=torch.long, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    return tensor, torch.tensor(label, dtype=torch.long, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")) #?\n",
    "    \n",
    "  def tensor2string(self, tensor):\n",
    "    ids = tensor.tolist()\n",
    "    string = ids2string(ids, rem_bos=True, rem_eos=True)\n",
    "    return string\n",
    "  \n",
    "  def forward(self, x, y):\n",
    "    z, kl_loss = self.forward_encoder(x, y)\n",
    "    recon_loss = self.forward_decoder(x, z, y)\n",
    "    return kl_loss, recon_loss\n",
    "  \n",
    "  def forward_encoder(self, x, y):\n",
    "    x = [self.x_emb(i_x) for i_x in x]\n",
    "    x = nn.utils.rnn.pack_sequence(x)\n",
    "    y = torch.tensor(y, dtype=torch.long, device=device)\n",
    "    y = torch.nn.functional.one_hot(y, self.n_classes).float() # 512, 2\n",
    "    _, h = self.encoder_rnn(x, None) # 2, 512, 256\n",
    "    h = h[-(1 + int(self.encoder_rnn.bidirectional)):] # 2, 512, 256\n",
    "    h = torch.cat(h.split(1), dim=-1).squeeze(0) \n",
    "    h = torch.cat((h, y), dim = -1) # 512, 514 \n",
    "    mu, logvar = self.q_mu(h), self.q_logvar(h) # 512, 128 \n",
    "    eps = torch.randn_like(mu)\n",
    "    z = mu + (logvar / 2).exp() * eps \n",
    "    kl_loss = 0.5 * (logvar.exp() + mu ** 2 - 1 - logvar).sum(1).mean()\n",
    "    return z, kl_loss\n",
    "  \n",
    "  def forward_decoder(self, x, z, y):\n",
    "    lengths = [len(i_x) for i_x in x]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value= c2i['<pad>'])\n",
    "    x_emb = self.x_emb(x) # 512, len, 60\n",
    "    y = torch.tensor(y, dtype=torch.long, device=device)\n",
    "    y = torch.nn.functional.one_hot(y, self.n_classes).float() \n",
    "    z_0 = z.unsqueeze(1).repeat(1, x_emb.size(1), 1) # 512, len, 128\n",
    "    x_input = torch.cat([x_emb, z_0], dim=-1) # 512, len, 60 + 128\n",
    "    x_input = nn.utils.rnn.pack_padded_sequence(x_input, lengths, batch_first=True)\n",
    "    h_0 = self.decoder_latent(z) # 512, 512 \n",
    "    h_0 = torch.cat((h_0, y), dim = -1) # 512, 514\n",
    "    h_0 = h_0.unsqueeze(0).repeat(self.decoder_rnn.num_layers, 1, 1) # 3, 512, 512\n",
    "    output, _ = self.decoder_rnn(x_input, h_0)\n",
    "    output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True) # 512, len, 512\n",
    "    y = self.decoder_fullyc(output) # 512, len, 60\n",
    "    recon_loss = F.cross_entropy(y[:, :-1].contiguous().view(-1, y.size(-1)),x[:, 1:].contiguous().view(-1),ignore_index= c2i['<pad>'])\n",
    "    return recon_loss\n",
    "  \n",
    "    \n",
    "  def sample_z_prior(self, n_batch):\n",
    "    return torch.randn(n_batch,self.q_mu.out_features, device=self.x_emb.weight.device)\n",
    "  \n",
    "  def sample(self, n_batch, label, max_len=100, z=None, temp=1.0):\n",
    "    with torch.no_grad():\n",
    "      if z is None:\n",
    "        z = self.sample_z_prior(n_batch) # 512, 128 \n",
    "        z = z.to(device)\n",
    "        z_0 = z.unsqueeze(1) # 512, 1, 128\n",
    "        h = self.decoder_latent(z) # 512, 512 \n",
    "        y = torch.tensor([label] * n_batch, dtype=torch.long, device=device)\n",
    "        y = torch.nn.functional.one_hot(y, self.n_classes).float()\n",
    "        h = torch.cat((h, y), dim = -1) # 512, 514 \n",
    "        h = h.unsqueeze(0).repeat(self.decoder_rnn.num_layers, 1, 1) # 3, 512, 512\n",
    "        w = torch.tensor(c2i['<bos>'], device=device).repeat(n_batch)\n",
    "        x = torch.tensor([c2i['<pad>']], device=device).repeat(n_batch, max_len)\n",
    "        x[:, 0] = c2i['<bos>']\n",
    "        end_pads = torch.tensor([max_len], device=device).repeat(n_batch)\n",
    "        eos_mask = torch.zeros(n_batch, dtype=torch.bool, device=device)\n",
    "\n",
    "        for i in range(1, max_len):\n",
    "          x_emb = self.x_emb(w).unsqueeze(1) # 512, 1, 60\n",
    "          x_input = torch.cat([x_emb, z_0], dim=-1) # 512, 1, 60 + 128\n",
    "          o, h = self.decoder_rnn(x_input, h)\n",
    "          y = self.decoder_fullyc(o.squeeze(1))\n",
    "          \n",
    "          y = F.softmax(y / temp, dim=-1)\n",
    "          y = torch.clamp(y, 1e-8, 1.0) \n",
    "          \n",
    "          w = torch.multinomial(y, 1)[:, 0]\n",
    "          x[~eos_mask, i] = w[~eos_mask]\n",
    "          i_eos_mask = ~eos_mask & (w == c2i['<eos>'])\n",
    "          end_pads[i_eos_mask] = i + 1\n",
    "          eos_mask = eos_mask | i_eos_mask\n",
    "\n",
    "        new_x = []\n",
    "        for i in range(x.size(0)):\n",
    "            new_x.append(x[i, :end_pads[i]])\n",
    "\n",
    "      return [self.tensor2string(i_x) for i_x in new_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_last = 1000\n",
    "n_batch = 512\n",
    "kl_start = 0\n",
    "kl_w_start = 0.0\n",
    "kl_w_end = 1.0\n",
    "n_epoch = 100\n",
    "n_workers = 0\n",
    "\n",
    "clip_grad  = 50\n",
    "lr_start = 0.003\n",
    "lr_n_period = 10\n",
    "lr_n_mult = 1\n",
    "lr_end = 3 * 1e-4\n",
    "lr_n_restarts = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_losses = pd.DataFrame(columns=['epoch', 'kl_weight', 'lr', 'kl_loss', 'recon_loss', 'loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLAnnealer:\n",
    "    def __init__(self,n_epoch):\n",
    "        self.i_start = kl_start\n",
    "        self.w_start = kl_w_start\n",
    "        self.w_max = kl_w_end\n",
    "        self.n_epoch = n_epoch\n",
    "\n",
    "        self.inc = (self.w_max - self.w_start) / (self.n_epoch - self.i_start)\n",
    "\n",
    "    def __call__(self, i):\n",
    "        k = (i - self.i_start) if i >= self.i_start else 0\n",
    "        return self.w_start + k * self.inc\n",
    "      \n",
    "      \n",
    "class CosineAnnealingLRWithRestart(_LRScheduler):\n",
    "    def __init__(self , optimizer):\n",
    "        self.n_period = lr_n_period\n",
    "        self.n_mult = lr_n_mult\n",
    "        self.lr_end = lr_end\n",
    "\n",
    "        self.current_epoch = 0\n",
    "        self.t_end = self.n_period\n",
    "\n",
    "        # Also calls first epoch\n",
    "        super().__init__(optimizer, -1)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.lr_end + (base_lr - self.lr_end) *\n",
    "                (1 + math.cos(math.pi * self.current_epoch / self.t_end)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "        self.current_epoch += 1\n",
    "\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        if self.current_epoch == self.t_end:\n",
    "            self.current_epoch = 0\n",
    "            self.t_end = self.n_mult * self.t_end         \n",
    "            \n",
    "            \n",
    "class CircularBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.max_size = size\n",
    "        self.data = np.zeros(self.max_size)\n",
    "        self.size = 0\n",
    "        self.pointer = -1\n",
    "\n",
    "    def add(self, element):\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        self.pointer = (self.pointer + 1) % self.max_size\n",
    "        self.data[self.pointer] = element\n",
    "        return element\n",
    "\n",
    "    def last(self):\n",
    "        assert self.pointer != -1, \"Can't get an element from an empty buffer!\"\n",
    "        return self.data[self.pointer]\n",
    "\n",
    "    def mean(self):\n",
    "        return self.data.mean()\n",
    "      \n",
    "      \n",
    "class Logger(UserList):\n",
    "    def __init__(self, data=None):\n",
    "        super().__init__()\n",
    "        self.sdata = defaultdict(list)\n",
    "        for step in (data or []):\n",
    "            self.append(step)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int):\n",
    "            return self.data[key]\n",
    "        elif isinstance(key, slice):\n",
    "            return Logger(self.data[key])\n",
    "        else:\n",
    "            ldata = self.sdata[key]\n",
    "            if isinstance(ldata[0], dict):\n",
    "                return Logger(ldata)\n",
    "            else:\n",
    "                return ldata\n",
    "\n",
    "    def append(self, step_dict):\n",
    "        super().append(step_dict)\n",
    "        for k, v in step_dict.items():\n",
    "            self.sdata[k].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _n_epoch():\n",
    "    return sum(lr_n_period * (lr_n_mult ** i) for i in range(lr_n_restarts))\n",
    "  \n",
    "def _train_epoch(model, epoch, train_loader, kl_weight, optimizer=None):\n",
    "    if optimizer is None:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "      \n",
    "    kl_loss_values = CircularBuffer(n_last)\n",
    "    recon_loss_values = CircularBuffer(n_last)\n",
    "    loss_values = CircularBuffer(n_last)\n",
    "\n",
    "    for data in train_loader:\n",
    "        input_batch, labels = [item[0] for item in data], [item[1] for item in data]\n",
    "        input_batch = tuple(data.to(device) for data in input_batch)\n",
    "        labels = [label.to(device) for label in labels]\n",
    "      \n",
    "    # forward\n",
    "        kl_loss, recon_loss = model(input_batch, labels)\n",
    "        loss = kl_weight * kl_loss + recon_loss\n",
    "    \n",
    "    # backward\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(get_optim_params(model),clip_grad)\n",
    "            optimizer.step()\n",
    "      \n",
    "        kl_loss_values.add(kl_loss.item())\n",
    "        recon_loss_values.add(recon_loss.item())\n",
    "        loss_values.add(loss.item())\n",
    "        lr = (optimizer.param_groups[0]['lr'] if optimizer is not None else None)\n",
    "      \n",
    "    # update train_loader\n",
    "        kl_loss_value = kl_loss_values.mean()\n",
    "        recon_loss_value = recon_loss_values.mean()\n",
    "        loss_value = loss_values.mean()\n",
    "        postfix = [f'loss={loss_value:.5f}',f'(kl={kl_loss_value:.5f}',f'recon={recon_loss_value:.5f})',f'klw={kl_weight:.5f} lr={lr:.5f}']\n",
    "    postfix = {'epoch': epoch,'kl_weight': kl_weight,'lr': lr,'kl_loss': kl_loss_value,'recon_loss': recon_loss_value,'loss': loss_value,'mode': 'Eval' if optimizer is None else 'Train'}\n",
    "    return postfix\n",
    "  \n",
    "def _train(model, train_loader, val_loader=None, logger=None):\n",
    "    optimizer = optim.Adam(get_optim_params(model), lr= lr_start)\n",
    "    \n",
    "    lr_annealer = CosineAnnealingLRWithRestart(optimizer)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    for epoch in tqdm(range(n_epoch), desc='Training', unit='epoch'):\n",
    "      \n",
    "        kl_annealer = KLAnnealer(n_epoch)\n",
    "        kl_weight = kl_annealer(epoch)\n",
    "        postfix = _train_epoch(model, epoch, train_loader, kl_weight, optimizer)\n",
    "        df_losses.loc[len(df_losses.index)] = [postfix['epoch'], postfix['kl_weight'], postfix['lr'], postfix['kl_loss'], postfix['recon_loss'], postfix['loss']]\n",
    "        lr_annealer.step()\n",
    "\n",
    "def fit(model, train_data, train_label, val_data=None):\n",
    "    logger = Logger() if False is not None else None\n",
    "    train_loader = get_dataloader(model, train_data, train_label, shuffle=True)\n",
    "    val_loader = None if val_data is None else get_dataloader(model, val_data, shuffle=False) # None\n",
    "    _train(model, train_loader, val_loader, logger)\n",
    "    return model\n",
    "\n",
    "def get_collate_device(model):\n",
    "    return model.device\n",
    "\n",
    "def get_dataloader(model, train_data, train_label, collate_fn=None, shuffle=True):\n",
    "    if collate_fn is None:\n",
    "        collate_fn = get_collate_fn(model)\n",
    "    return DataLoader(list(zip(train_data, train_label)), batch_size=n_batch, shuffle=shuffle, num_workers=n_workers, collate_fn=collate_fn)\n",
    "\n",
    "def get_collate_fn(model):\n",
    "    device = get_collate_device(model)\n",
    "\n",
    "    def collate(train_data):\n",
    "        train_data.sort(key=lambda x: len(x[0]), reverse=True) #?\n",
    "        tensors = [model.string2tensor(string, label, device=device) for string, label in train_data]\n",
    "        return tensors \n",
    "\n",
    "    return collate\n",
    "\n",
    "def get_optim_params(model):\n",
    "    return (p for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_samples = 1000\n",
    "n_jobs = 1\n",
    "max_len = 100\n",
    "label = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample():\n",
    "  def take_samples(model, n_batch, label):\n",
    "    n = n_samples\n",
    "    samples = []\n",
    "    with tqdm(total=n_samples, desc='Generating samples') as T:\n",
    "      while n > 0:\n",
    "        current_samples = model.sample(min(n, n_batch), label, max_len)\n",
    "        samples.extend(current_samples)\n",
    "        n -= len(current_samples)\n",
    "        T.update(len(current_samples))\n",
    "    samples = pd.DataFrame(samples, columns=['SMILES'])\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [1:57:20<00:00, 70.40s/epoch]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CVAE(\n",
       "  (x_emb): Embedding(60, 60, padding_idx=58)\n",
       "  (encoder_rnn): GRU(60, 256, batch_first=True, bidirectional=True)\n",
       "  (q_mu): Linear(in_features=514, out_features=128, bias=True)\n",
       "  (q_logvar): Linear(in_features=514, out_features=128, bias=True)\n",
       "  (decoder_rnn): GRU(188, 514, num_layers=3, batch_first=True)\n",
       "  (decoder_latent): Linear(in_features=128, out_features=512, bias=True)\n",
       "  (decoder_fullyc): Linear(in_features=514, out_features=60, bias=True)\n",
       "  (encoder): ModuleList(\n",
       "    (0): GRU(60, 256, batch_first=True, bidirectional=True)\n",
       "    (1-2): 2 x Linear(in_features=514, out_features=128, bias=True)\n",
       "  )\n",
       "  (decoder): ModuleList(\n",
       "    (0): GRU(188, 514, num_layers=3, batch_first=True)\n",
       "    (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (2): Linear(in_features=514, out_features=60, bias=True)\n",
       "  )\n",
       "  (vae): ModuleList(\n",
       "    (0): Embedding(60, 60, padding_idx=58)\n",
       "    (1): ModuleList(\n",
       "      (0): GRU(60, 256, batch_first=True, bidirectional=True)\n",
       "      (1-2): 2 x Linear(in_features=514, out_features=128, bias=True)\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0): GRU(188, 514, num_layers=3, batch_first=True)\n",
       "      (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (2): Linear(in_features=514, out_features=60, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CVAE(vocab, vector).to(device)\n",
    "fit(model, train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), f'checkpoints/cvae_model_epoch{n_epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model state dictionary\n",
    "model = CVAE(vocab, vector).to(device)\n",
    "model.load_state_dict(torch.load(f'checkpoints/cvae_model_epoch{n_epoch}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples: 100%|██████████| 1000/1000 [00:00<00:00, 1176.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N#CC1=C(N)OC(c2ccc([N+](=O)[O-])cc2)O1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC1(C)CC(=O)C2=C(C1)Nc1ccccc1NC1=NCC21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCCCCCC1=C(O)C(=NCCN)C(C(=O)OC)C1N(Cc1ccccc1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COc1ccc(C=Cc2ccc3cccnc3c2O)cc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CSc1ncnc2c1cnn2CCCCCCCCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Cc1occc1C(=S)Nc1ccc(Cl)c(C(=O)OC2CCCC2)c1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>O=S(=O)(O)c1cc(NN=Cc2cc(S(=O)(=O)O)c3cccnc3c2O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>CC(NC(=O)C1CC(=O)NC(=O)C1C)C(=O)O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>CCOC(=O)C=C1C(=O)Oc2cc(=O)cc(OC(C)=O)c21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Cc1cc2nc(C3CCCCC3)nc(N)c2c(=O)n1-c1ccc(CN)cc1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                SMILES\n",
       "0               N#CC1=C(N)OC(c2ccc([N+](=O)[O-])cc2)O1\n",
       "1               CC1(C)CC(=O)C2=C(C1)Nc1ccccc1NC1=NCC21\n",
       "2    CCCCCCCCC1=C(O)C(=NCCN)C(C(=O)OC)C1N(Cc1ccccc1...\n",
       "3                       COc1ccc(C=Cc2ccc3cccnc3c2O)cc1\n",
       "4                             CSc1ncnc2c1cnn2CCCCCCCCO\n",
       "..                                                 ...\n",
       "995          Cc1occc1C(=S)Nc1ccc(Cl)c(C(=O)OC2CCCC2)c1\n",
       "996  O=S(=O)(O)c1cc(NN=Cc2cc(S(=O)(=O)O)c3cccnc3c2O...\n",
       "997                  CC(NC(=O)C1CC(=O)NC(=O)C1C)C(=O)O\n",
       "998           CCOC(=O)C=C1C(=O)Oc2cc(=O)cc(OC(C)=O)c21\n",
       "999      Cc1cc2nc(C3CCCCC3)nc(N)c2c(=O)n1-c1ccc(CN)cc1\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample SMILES\n",
    "model.eval()\n",
    "df_sample = sample.take_samples(model, n_batch, label)\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generated molecules\n",
    "df_sample.to_csv(f'generated_molecules/cvae/cvae_epoch{n_epoch}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_losses.to_csv(f'checkpoints/losses/cvae_epoch{n_epoch}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
