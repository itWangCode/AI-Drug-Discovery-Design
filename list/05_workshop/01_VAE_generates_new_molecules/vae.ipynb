{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:18:27.838833Z",
     "start_time": "2024-10-16T18:18:06.403639Z"
    }
   },
   "source": [
    "import deepchem as dc\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from collections import UserList, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rdkit import RDLogger                                                                                                                                                               \n",
    "RDLogger.DisableLog('rdApp.*') \n",
    "\n",
    "def seed_all():\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 02:18:20.962412: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:18:33.472487Z",
     "start_time": "2024-10-16T18:18:33.467877Z"
    }
   },
   "source": [
    "seed_all()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:21:41.940794Z",
     "start_time": "2024-10-16T18:18:34.864386Z"
    }
   },
   "source": [
    "# Load HIV dataset\n",
    "hiv_tasks, datasets, transformers = dc.molnet.load_hiv(featurizer=\"ECFP\", set=\"sparse\", splitter = 'random')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:21:41.966856Z",
     "start_time": "2024-10-16T18:21:41.952209Z"
    }
   },
   "source": [
    "# Split 80/10/10\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "print(f'Compound train/valid/test split: {len(train_dataset)}/{len(valid_dataset)}/{len(test_dataset)}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compound train/valid/test split: 32901/4113/4113\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:22:02.731067Z",
     "start_time": "2024-10-16T18:22:02.692556Z"
    }
   },
   "source": [
    "# Select SMILES structures and label\n",
    "train_data = list(train_dataset.ids)\n",
    "train_label = list(train_dataset.y)\n",
    "print(f'Example SMILES: {train_data[0]}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example SMILES: O=C1C(=Cc2ccc(O)cc2)N=C(c2ccccc2)N1n1c(-c2ccccc2)nc2ccccc2c1=O\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:22:05.092691Z",
     "start_time": "2024-10-16T18:22:04.411024Z"
    }
   },
   "source": [
    "chars = set()\n",
    "for string in train_data:\n",
    "    chars.update(string)\n",
    "all_sys = sorted(list(chars)) + ['<bos>', '<eos>', '<pad>', '<unk>']\n",
    "vocab = all_sys\n",
    "c2i = {c: i for i, c in enumerate(all_sys)}\n",
    "i2c = {i: c for i, c in enumerate(all_sys)}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vector = torch.eye(len(c2i))\n",
    "\n",
    "\n",
    "def char2id(char):\n",
    "    if char not in c2i:\n",
    "        return c2i['<unk>']\n",
    "    else:\n",
    "        return c2i[char]\n",
    "\n",
    "\n",
    "def id2char(id):\n",
    "    if id not in i2c:\n",
    "        return i2c[32]\n",
    "    else:\n",
    "        return i2c[id]\n",
    "\n",
    "def string2ids(string,add_bos=False, add_eos=False):\n",
    "    ids = [char2id(c) for c in string]\n",
    "    if add_bos:\n",
    "        ids = [c2i['<bos>']] + ids\n",
    "    if add_eos:\n",
    "        ids = ids + [c2i['<eos>']]\n",
    "    return ids\n",
    "\n",
    "def ids2string(ids, rem_bos=True, rem_eos=True):\n",
    "    if len(ids) == 0:\n",
    "        return ''\n",
    "    if rem_bos and ids[0] == c2i['<bos>']:\n",
    "        ids = ids[1:]\n",
    "    if rem_eos and ids[-1] == c2i['<eos>']:\n",
    "        ids = ids[:-1]\n",
    "    string = ''.join([id2char(id) for id in ids])\n",
    "    return string\n",
    "\n",
    "def string2tensor(string, device='model'):\n",
    "    ids = string2ids(string, add_bos=True, add_eos=True)\n",
    "    tensor = torch.tensor(ids, dtype=torch.long, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    return tensor\n",
    "\n",
    "tensor = [string2tensor(string, device=device) for string in train_data]\n",
    "vector = torch.eye(len(c2i))"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:22:07.013879Z",
     "start_time": "2024-10-16T18:22:07.008498Z"
    }
   },
   "source": [
    "# Parameters\n",
    "q_bidir = True\n",
    "q_d_h = 256\n",
    "q_n_layers = 1\n",
    "q_dropout = 0.5\n",
    "d_n_layers = 3\n",
    "d_dropout = 0\n",
    "d_z = 128\n",
    "d_d_h = 512"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:22:07.829547Z",
     "start_time": "2024-10-16T18:22:07.805851Z"
    }
   },
   "source": [
    "class VAE(nn.Module):\n",
    "  def __init__(self,vocab,vector):\n",
    "    super().__init__()\n",
    "    self.vocabulary = vocab\n",
    "    self.vector = vector\n",
    "    \n",
    "    n_vocab, d_emb = len(vocab), vector.size(1)\n",
    "    self.x_emb = nn.Embedding(n_vocab, d_emb, c2i['<pad>'])\n",
    "    self.x_emb.weight.data.copy_(vector)\n",
    "  \n",
    "    #Encoder\n",
    "    self.encoder_rnn = nn.GRU(d_emb,q_d_h,num_layers=q_n_layers,batch_first=True,dropout=q_dropout if q_n_layers > 1 else 0,bidirectional=q_bidir)\n",
    "    q_d_last = q_d_h * (2 if q_bidir else 1)\n",
    "    self.q_mu = nn.Linear(q_d_last, d_z)\n",
    "    self.q_logvar = nn.Linear(q_d_last, d_z)\n",
    "  \n",
    "    # Decoder\n",
    "    self.decoder_rnn = nn.GRU(d_emb + d_z,d_d_h,num_layers=d_n_layers,batch_first=True,dropout=d_dropout if d_n_layers > 1 else 0)\n",
    "    self.decoder_latent = nn.Linear(d_z, d_d_h)\n",
    "    self.decoder_fullyc = nn.Linear(d_d_h, n_vocab)\n",
    "  \n",
    "  \n",
    "    # Grouping the model's parameters\n",
    "    self.encoder = nn.ModuleList([self.encoder_rnn,self.q_mu,self.q_logvar])\n",
    "    self.decoder = nn.ModuleList([self.decoder_rnn,self.decoder_latent,self.decoder_fullyc])\n",
    "    self.vae = nn.ModuleList([self.x_emb,self.encoder,self.decoder])\n",
    "    \n",
    "\n",
    "  def device(self):\n",
    "    return next(self.parameters()).device\n",
    "\n",
    "  def string2tensor(self, string, device='model'):\n",
    "    ids = string2ids(string, add_bos=True, add_eos=True)\n",
    "    tensor = torch.tensor(ids, dtype=torch.long, device=self.device if device == 'model' else device)\n",
    "    return tensor\n",
    "\n",
    "  def tensor2string(self, tensor):\n",
    "    ids = tensor.tolist()\n",
    "    string = ids2string(ids, rem_bos=True, rem_eos=True)\n",
    "    return string\n",
    "  \n",
    "  def forward(self,x):\n",
    "    z, kl_loss = self.forward_encoder(x)\n",
    "    recon_loss = self.forward_decoder(x, z)\n",
    "    return kl_loss, recon_loss\n",
    "  \n",
    "  def forward_encoder(self,x):\n",
    "    x = [self.x_emb(i_x) for i_x in x]\n",
    "    x = nn.utils.rnn.pack_sequence(x)\n",
    "    _, h = self.encoder_rnn(x, None)\n",
    "    h = h[-(1 + int(self.encoder_rnn.bidirectional)):]\n",
    "    h = torch.cat(h.split(1), dim=-1).squeeze(0)\n",
    "    mu, logvar = self.q_mu(h), self.q_logvar(h)\n",
    "    eps = torch.randn_like(mu)\n",
    "    z = mu + (logvar / 2).exp() * eps\n",
    "    kl_loss = 0.5 * (logvar.exp() + mu ** 2 - 1 - logvar).sum(1).mean()\n",
    "    return z, kl_loss\n",
    "  \n",
    "  def forward_decoder(self,x, z):\n",
    "    lengths = [len(i_x) for i_x in x]\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value= c2i['<pad>'])\n",
    "    x_emb = self.x_emb(x)\n",
    "    z_0 = z.unsqueeze(1).repeat(1, x_emb.size(1), 1)\n",
    "    x_input = torch.cat([x_emb, z_0], dim=-1)\n",
    "    x_input = nn.utils.rnn.pack_padded_sequence(x_input, lengths, batch_first=True)\n",
    "    h_0 = self.decoder_latent(z)\n",
    "    h_0 = h_0.unsqueeze(0).repeat(self.decoder_rnn.num_layers, 1, 1)\n",
    "    output, _ = self.decoder_rnn(x_input, h_0)\n",
    "    output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "    y = self.decoder_fullyc(output)\n",
    "    \n",
    "    recon_loss = F.cross_entropy(y[:, :-1].contiguous().view(-1, y.size(-1)),x[:, 1:].contiguous().view(-1),ignore_index= c2i['<pad>'])\n",
    "    return recon_loss\n",
    "  \n",
    "    \n",
    "  def sample_z_prior(self,n_batch):\n",
    "    return torch.randn(n_batch,self.q_mu.out_features, device=self.x_emb.weight.device)\n",
    "  \n",
    "  def sample(self, n_batch, max_len=100, z=None, temp=1.0):\n",
    "    with torch.no_grad():\n",
    "      if z is None:\n",
    "        z = self.sample_z_prior(n_batch)\n",
    "        z = z.to(device)\n",
    "        z_0 = z.unsqueeze(1)\n",
    "        h = self.decoder_latent(z)\n",
    "        h = h.unsqueeze(0).repeat(self.decoder_rnn.num_layers, 1, 1)\n",
    "        w = torch.tensor(c2i['<bos>'], device=device).repeat(n_batch)\n",
    "        x = torch.tensor([c2i['<pad>']], device=device).repeat(n_batch, max_len)\n",
    "        x[:, 0] = c2i['<bos>']\n",
    "        end_pads = torch.tensor([max_len], device=device).repeat(n_batch)\n",
    "        eos_mask = torch.zeros(n_batch, dtype=torch.bool, device=device)\n",
    "\n",
    "        for i in range(1, max_len):\n",
    "          x_emb = self.x_emb(w).unsqueeze(1)\n",
    "          x_input = torch.cat([x_emb, z_0], dim=-1)\n",
    "\n",
    "          o, h = self.decoder_rnn(x_input, h)\n",
    "          y = self.decoder_fullyc(o.squeeze(1))\n",
    "          \n",
    "          y = F.softmax(y / temp, dim=-1)\n",
    "          y = torch.clamp(y, 1e-8, 1.0) \n",
    "          \n",
    "          w = torch.multinomial(y, 1)[:, 0]\n",
    "          x[~eos_mask, i] = w[~eos_mask]\n",
    "          i_eos_mask = ~eos_mask & (w == c2i['<eos>'])\n",
    "          end_pads[i_eos_mask] = i + 1\n",
    "          eos_mask = eos_mask | i_eos_mask\n",
    "\n",
    "        new_x = []\n",
    "        for i in range(x.size(0)):\n",
    "            new_x.append(x[i, :end_pads[i]])\n",
    "\n",
    "      return [self.tensor2string(i_x) for i_x in new_x]"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:22:09.738819Z",
     "start_time": "2024-10-16T18:22:09.734326Z"
    }
   },
   "source": [
    "# Parameters\n",
    "n_last = 1000\n",
    "n_batch = 512\n",
    "kl_start = 0\n",
    "kl_w_start = 0.0\n",
    "kl_w_end = 1.0\n",
    "n_epoch = 100\n",
    "n_workers = 0\n",
    "\n",
    "clip_grad  = 50\n",
    "lr_start = 0.003\n",
    "lr_n_period = 10\n",
    "lr_n_mult = 1\n",
    "lr_end = 3 * 1e-4\n",
    "lr_n_restarts = 6"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:22:10.597457Z",
     "start_time": "2024-10-16T18:22:10.590509Z"
    }
   },
   "source": [
    "df_losses = pd.DataFrame(columns=['epoch', 'kl_weight', 'lr', 'kl_loss', 'recon_loss', 'loss'])"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:22:12.402333Z",
     "start_time": "2024-10-16T18:22:12.387618Z"
    }
   },
   "source": [
    "class KLAnnealer:\n",
    "    def __init__(self,n_epoch):\n",
    "        self.i_start = kl_start\n",
    "        self.w_start = kl_w_start\n",
    "        self.w_max = kl_w_end\n",
    "        self.n_epoch = n_epoch\n",
    "\n",
    "        self.inc = (self.w_max - self.w_start) / (self.n_epoch - self.i_start)\n",
    "\n",
    "    def __call__(self, i):\n",
    "        k = (i - self.i_start) if i >= self.i_start else 0\n",
    "        return self.w_start + k * self.inc\n",
    "      \n",
    "      \n",
    "class CosineAnnealingLRWithRestart(_LRScheduler):\n",
    "    def __init__(self , optimizer):\n",
    "        self.n_period = lr_n_period\n",
    "        self.n_mult = lr_n_mult\n",
    "        self.lr_end = lr_end\n",
    "\n",
    "        self.current_epoch = 0\n",
    "        self.t_end = self.n_period\n",
    "\n",
    "        # Also calls first epoch\n",
    "        super().__init__(optimizer, -1)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.lr_end + (base_lr - self.lr_end) *\n",
    "                (1 + math.cos(math.pi * self.current_epoch / self.t_end)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "        self.current_epoch += 1\n",
    "\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        if self.current_epoch == self.t_end:\n",
    "            self.current_epoch = 0\n",
    "            self.t_end = self.n_mult * self.t_end         \n",
    "            \n",
    "            \n",
    "class CircularBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.max_size = size\n",
    "        self.data = np.zeros(self.max_size)\n",
    "        self.size = 0\n",
    "        self.pointer = -1\n",
    "\n",
    "    def add(self, element):\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        self.pointer = (self.pointer + 1) % self.max_size\n",
    "        self.data[self.pointer] = element\n",
    "        return element\n",
    "\n",
    "    def last(self):\n",
    "        assert self.pointer != -1, \"Can't get an element from an empty buffer!\"\n",
    "        return self.data[self.pointer]\n",
    "\n",
    "    def mean(self):\n",
    "        return self.data.mean()\n",
    "      \n",
    "      \n",
    "class Logger(UserList):\n",
    "    def __init__(self, data=None):\n",
    "        super().__init__()\n",
    "        self.sdata = defaultdict(list)\n",
    "        for step in (data or []):\n",
    "            self.append(step)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int):\n",
    "            return self.data[key]\n",
    "        elif isinstance(key, slice):\n",
    "            return Logger(self.data[key])\n",
    "        else:\n",
    "            ldata = self.sdata[key]\n",
    "            if isinstance(ldata[0], dict):\n",
    "                return Logger(ldata)\n",
    "            else:\n",
    "                return ldata\n",
    "\n",
    "    def append(self, step_dict):\n",
    "        super().append(step_dict)\n",
    "        for k, v in step_dict.items():\n",
    "            self.sdata[k].append(v)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:22:13.319032Z",
     "start_time": "2024-10-16T18:22:13.306682Z"
    }
   },
   "source": [
    "def _n_epoch():\n",
    "    return sum(lr_n_period * (lr_n_mult ** i) for i in range(lr_n_restarts))\n",
    "  \n",
    "def _train_epoch(model, epoch, train_loader, kl_weight, optimizer=None):\n",
    "    if optimizer is None:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "      \n",
    "    kl_loss_values = CircularBuffer(n_last)\n",
    "    recon_loss_values = CircularBuffer(n_last)\n",
    "    loss_values = CircularBuffer(n_last)\n",
    "    for i, input_batch in enumerate(train_loader):\n",
    "        input_batch = tuple(data.to(device) for data in input_batch)\n",
    "      \n",
    "    # forward\n",
    "        kl_loss, recon_loss = model(input_batch)\n",
    "        loss = kl_weight * kl_loss + recon_loss\n",
    "    # backward\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(get_optim_params(model),clip_grad)\n",
    "            optimizer.step()\n",
    "      \n",
    "        kl_loss_values.add(kl_loss.item())\n",
    "        recon_loss_values.add(recon_loss.item())\n",
    "        loss_values.add(loss.item())\n",
    "        lr = (optimizer.param_groups[0]['lr'] if optimizer is not None else None)\n",
    "      \n",
    "    # update train_loader\n",
    "        kl_loss_value = kl_loss_values.mean()\n",
    "        recon_loss_value = recon_loss_values.mean()\n",
    "        loss_value = loss_values.mean()\n",
    "        postfix = [f'loss={loss_value:.5f}',f'(kl={kl_loss_value:.5f}',f'recon={recon_loss_value:.5f})',f'klw={kl_weight:.5f} lr={lr:.5f}']\n",
    "    postfix = {'epoch': epoch,'kl_weight': kl_weight,'lr': lr,'kl_loss': kl_loss_value,'recon_loss': recon_loss_value,'loss': loss_value,'mode': 'Eval' if optimizer is None else 'Train'}\n",
    "    return postfix\n",
    "  \n",
    "def _train(model, train_loader, val_loader=None, logger=None):\n",
    "    optimizer = optim.Adam(get_optim_params(model),lr= lr_start)\n",
    "    \n",
    "    lr_annealer = CosineAnnealingLRWithRestart(optimizer)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    for epoch in tqdm(range(n_epoch), desc='Training', unit='epoch'):\n",
    "      \n",
    "        kl_annealer = KLAnnealer(n_epoch)\n",
    "        kl_weight = kl_annealer(epoch)\n",
    "        postfix = _train_epoch(model, epoch,train_loader, kl_weight, optimizer)\n",
    "        df_losses.loc[len(df_losses.index)] = [postfix['epoch'], postfix['kl_weight'], postfix['lr'], postfix['kl_loss'], postfix['recon_loss'], postfix['loss']]\n",
    "        lr_annealer.step()\n",
    "\n",
    "def fit(model, train_data, val_data=None):\n",
    "    logger = Logger() if False is not None else None\n",
    "    train_loader = get_dataloader(model,train_data,shuffle=True)\n",
    "    val_loader = None if val_data is None else get_dataloader(model, val_data, shuffle=False)\n",
    "    _train(model, train_loader, val_loader, logger)\n",
    "    return model\n",
    "\n",
    "def get_collate_device(model):\n",
    "    return model.device\n",
    "\n",
    "def get_dataloader(model, train_data, collate_fn=None, shuffle=True):\n",
    "    if collate_fn is None:\n",
    "        collate_fn = get_collate_fn(model)\n",
    "    return DataLoader(train_data, batch_size=n_batch, shuffle=shuffle, num_workers=n_workers, collate_fn=collate_fn)\n",
    "\n",
    "def get_collate_fn(model):\n",
    "    device = get_collate_device(model)\n",
    "\n",
    "    def collate(train_data):\n",
    "        train_data.sort(key=len, reverse=True)\n",
    "        tensors = [string2tensor(string, device=device) for string in train_data]\n",
    "        return tensors\n",
    "\n",
    "    return collate\n",
    "\n",
    "def get_optim_params(model):\n",
    "    return (p for p in model.parameters() if p.requires_grad)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample from model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:22:15.172072Z",
     "start_time": "2024-10-16T18:22:15.169282Z"
    }
   },
   "source": [
    "# Parameters\n",
    "n_samples = 1000\n",
    "n_jobs = 1\n",
    "max_len = 100"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T18:22:15.791145Z",
     "start_time": "2024-10-16T18:22:15.786632Z"
    }
   },
   "source": [
    "class sample():\n",
    "  def take_samples(model, n_batch):\n",
    "    n = n_samples\n",
    "    samples = []\n",
    "    with tqdm(total=n_samples, desc='Generating samples') as T:\n",
    "      while n > 0:\n",
    "        current_samples = model.sample(min(n, n_batch), max_len)\n",
    "        samples.extend(current_samples)\n",
    "        n -= len(current_samples)\n",
    "        T.update(len(current_samples))\n",
    "    samples = pd.DataFrame(samples, columns=['SMILES'])\n",
    "    return samples"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-16T18:22:18.136978Z"
    }
   },
   "source": [
    "model = VAE(vocab, vector).to(device)\n",
    "fit(model, train_data)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/100 [00:00<?, ?epoch/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), f'checkpoints/vae_model_epoch{n_epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model state dictionary\n",
    "model = VAE(vocab, vector).to(device)\n",
    "model.load_state_dict(torch.load(f'checkpoints/vae_model_epoch{n_epoch}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples: 100%|██████████| 1000/1000 [00:00<00:00, 1396.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O=C(CC1(c2ccccc2)NC(=O)C1CCCC(=O)O)CCCCC1=Nc2c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O=C(CCCN1CCC(N2CCN(c3ccccc3)CC2)N=C1)c1ccncc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCN(CCCC)C(=O)Cc1c2n(c3ccccc13)CCc1ccc3c(=O)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COC(=O)C1=CN(Cc2ccccc2)CC(=O)C2CC1C1CCC2(CC(=O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O=C(NCCCN1CCOCC1)c1cc2cc(F)ccc2nc1O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>CC(=O)N(c1ccccc1)c1c(O)c2ccccc2oc1=O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>CC(=O)OCC1OC(n2c(=O)n(-c3ccccc3)c(=O)c3cc(Br)c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>COc1ccccc1N1C(=O)C(=Cc2ccc(Oc3ccccc3)cc1)N(CC)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>O=C(c1ccccc1)C(c1ccccc1)C(O)(c1ccccc1)c1ccccc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>O=[N+]([O-])c1cccc2nsnc12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                SMILES\n",
       "0    O=C(CC1(c2ccccc2)NC(=O)C1CCCC(=O)O)CCCCC1=Nc2c...\n",
       "1        O=C(CCCN1CCC(N2CCN(c3ccccc3)CC2)N=C1)c1ccncc1\n",
       "2    CCCCN(CCCC)C(=O)Cc1c2n(c3ccccc13)CCc1ccc3c(=O)...\n",
       "3    COC(=O)C1=CN(Cc2ccccc2)CC(=O)C2CC1C1CCC2(CC(=O...\n",
       "4                  O=C(NCCCN1CCOCC1)c1cc2cc(F)ccc2nc1O\n",
       "..                                                 ...\n",
       "995               CC(=O)N(c1ccccc1)c1c(O)c2ccccc2oc1=O\n",
       "996  CC(=O)OCC1OC(n2c(=O)n(-c3ccccc3)c(=O)c3cc(Br)c...\n",
       "997  COc1ccccc1N1C(=O)C(=Cc2ccc(Oc3ccccc3)cc1)N(CC)...\n",
       "998     O=C(c1ccccc1)C(c1ccccc1)C(O)(c1ccccc1)c1ccccc1\n",
       "999                          O=[N+]([O-])c1cccc2nsnc12\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample SMILES\n",
    "model.eval()\n",
    "df_sample = sample.take_samples(model, n_batch)\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generated molecules\n",
    "df_sample.to_csv(f'generated_molecules/vae/vae_epoch{n_epoch}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_losses.to_csv(f'checkpoints/losses/vae_epoch{n_epoch}.csv', index = False)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
